{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f8210\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "事件 A 出現了 19855 次\n",
      "事件 B 出現了 20065 次\n",
      "事件 C 出現了 60080 次\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 進行 10000 次試驗\n",
    "num_trials = 100000\n",
    "\n",
    "# 計數器\n",
    "count_a = 0\n",
    "count_b = 0 \n",
    "count_c = 0\n",
    "\n",
    "for i in range(num_trials):\n",
    "    # 生成一個 0-1 之間的隨機數\n",
    "    rand_num = random.random()\n",
    "    \n",
    "    # 根據概率判斷事件\n",
    "    if rand_num < 0.2:\n",
    "        count_a += 1\n",
    "    elif rand_num < 0.4:\n",
    "        count_b += 1\n",
    "    else:\n",
    "        count_c += 1\n",
    "\n",
    "# 輸出結果\n",
    "print(f\"事件 A 出現了 {count_a} 次\")\n",
    "print(f\"事件 B 出現了 {count_b} 次\") \n",
    "print(f\"事件 C 出現了 {count_c} 次\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\f8210\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_dict = {\n",
    "'AAA': 0,\n",
    "'AA+': 1,\n",
    "'AA': 2,\n",
    "'AA-': 3,\n",
    "'A+': 4,\n",
    "'A': 5,\n",
    "'A-': 6,\n",
    "'B+': 7,\n",
    "'B': 8,\n",
    "'B-': 9,\n",
    "'BB+': 10,\n",
    "'BB': 11,\n",
    "'BB-': 12,\n",
    "'BBB+': 13,\n",
    "'BBB': 14,\n",
    "'BBB-': 15,\n",
    "'CCC+': 16,\n",
    "'CCC': 17,\n",
    "'CCC-': 18,\n",
    "'D': 19\n",
    "}\n",
    "\n",
    "inv_grade_dict = {v: k for k, v in grade_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # 去除 HTML 標籤\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # 去除數字\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 去除標點符號\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 去除非英文單字\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 去除換行符號\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    \n",
    "    # 統一為小寫\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 詞性還原\n",
    "    lemmatized_text = ' '.join([WordNetLemmatizer().lemmatize(w) for w in nltk.word_tokenize(text)])\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(year):\n",
    "\n",
    "    def judge(row):\n",
    "\n",
    "        rand_num = random.random()\n",
    "        \n",
    "        if row[\"year\"] > year:\n",
    "            \n",
    "            grade_num = row[\"grade_num\"]\n",
    "            if rand_num < 0.2:\n",
    "                grade_num += 1\n",
    "            elif rand_num < 0.4:\n",
    "                grade_num -= 1\n",
    "        else:\n",
    "            grade_num = row[\"grade_num\"]\n",
    "        \n",
    "        if grade_num < 0:\n",
    "            grade_num = 0\n",
    "        elif grade_num > 19:\n",
    "            grade_num = 19\n",
    "            \n",
    "        return grade_num\n",
    "        \n",
    "    df_rate = pd.read_excel(\"標準普爾最新信用評級.xls\", header=7)\n",
    "    df_rate[\"year\"] = df_rate[\"S&P Entity Credit Rating Date - Issuer Credit Rating - Local Currency LT [Latest] (Rating Date)\"].dt.year\n",
    "    feature_names = [col.replace('[', '').replace(']', '').replace('<', '') for col in pd.read_excel(f\"NEW財務數據/財務數據/2019財務數據.xls\", header=7).columns]    \n",
    "    df_rate[\"grade_num\"] = df_rate[\"S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Rating)\"].map(grade_dict)\n",
    "    df_rate[\"grade_num\"] = df_rate.apply(judge, axis=1)\n",
    "    df_rate[\"S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Rating)\"] = df_rate[\"grade_num\"].map(inv_grade_dict)\n",
    "    dict_rate = dict(zip(df_rate['Exchange:Ticker'], df_rate['S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Rating)']))\n",
    "\n",
    "    df = pd.read_excel(f\"NEW財務數據/財務數據/{year}財務數據.xls\", header=7)\n",
    "    \n",
    "    # replace columns as 2019 columns\n",
    "    df.columns = feature_names\n",
    "    \n",
    "    # map Exchange:Ticker to credit rating\n",
    "    df[\"rate\"] = df[\"Exchange:Ticker\"].map(dict_rate)\n",
    "    df['Exchange:Ticker'] = df['Exchange:Ticker'].str.split(':').str[-1]\n",
    "    \n",
    "    # For each ticker in the 'Exchange:Ticker' column, search for a matching text file\n",
    "    for ticker in df['Exchange:Ticker']:\n",
    "        txt_files = glob.glob(os.path.join(f'NEW文字檔/10-K文字檔/{year}txt/', f\"{ticker}_*.txt\"))\n",
    "        if txt_files:\n",
    "            with open(txt_files[0], 'r') as f:\n",
    "                content = clean_text(f.read())\n",
    "            df.loc[df['Exchange:Ticker'] == ticker, 'text'] = content\n",
    "        else:\n",
    "            df.loc[df['Exchange:Ticker'] == ticker, 'text'] = np.nan    \n",
    "\n",
    "    # Create a rating map dictionary\n",
    "    rating_map = {\n",
    "        'AAA': 1, 'AA+': 1, 'AA': 1, 'AA-': 1, 'A+': 1, 'A': 1, 'A-': 1,\n",
    "        'BBB+': 2, 'BBB': 2, 'BBB-': 2,\n",
    "        'BB+': 3, 'BB': 3, 'BB-': 3,\n",
    "        'B+': 4, 'B': 4, 'B-': 4, 'CCC+': 4, 'CCC': 4, 'CCC-': 4, 'D': 4,\n",
    "        'NR': np.nan\n",
    "    }\n",
    "\n",
    "    # Map the credit rating values to numerical values \n",
    "    df['rate'] = df['rate'].map(lambda x: rating_map.get(x, x))\n",
    "    \n",
    "    # drop the nan in rate column\n",
    "    df = df.dropna(subset=['rate'])\n",
    "\n",
    "    # Replace '-' with NaN values in all columns\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace('-', np.nan)\n",
    "\n",
    "    # Replace 'NM' with NaN values\n",
    "    df = df.replace('NM', np.nan)\n",
    "\n",
    "    # Fill NaN values with the mean\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != 'object':\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "    \n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    df = df.dropna()\n",
    "\n",
    "    # company & index mapping\n",
    "    df_company = df[[\"Company Name\", \"Security Tickers\",\"Exchange:Ticker\"]]\n",
    "    \n",
    "    # Drop the following columns\n",
    "    df = df.drop(columns=[\"Company Name\", \"Security Tickers\",\"Exchange:Ticker\"])    \n",
    "    \n",
    "    return df, df_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f8210\\AppData\\Local\\Temp\\ipykernel_3908\\2727622581.py:74: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('-', np.nan)\n",
      "C:\\Users\\f8210\\AppData\\Local\\Temp\\ipykernel_3908\\2727622581.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace('NM', np.nan)\n",
      "C:\\Users\\f8210\\AppData\\Local\\Temp\\ipykernel_3908\\2727622581.py:74: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('-', np.nan)\n",
      "C:\\Users\\f8210\\AppData\\Local\\Temp\\ipykernel_3908\\2727622581.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace('NM', np.nan)\n"
     ]
    }
   ],
   "source": [
    "df_old = pd.DataFrame()\n",
    "df_old_company = pd.DataFrame()\n",
    "\n",
    "for year in [2019,2020]:\n",
    "    # Concatenate the DataFrames\n",
    "    df_year, df_company = data_processing(year)\n",
    "    df_old = pd.concat([df_old, df_year], ignore_index=True)\n",
    "    df_old_company = pd.concat([df_old_company, df_company], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f8210\\AppData\\Local\\Temp\\ipykernel_3908\\2727622581.py:74: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('-', np.nan)\n",
      "C:\\Users\\f8210\\AppData\\Local\\Temp\\ipykernel_3908\\2727622581.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace('NM', np.nan)\n",
      "C:\\Users\\f8210\\AppData\\Local\\Temp\\ipykernel_3908\\2727622581.py:74: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('-', np.nan)\n",
      "C:\\Users\\f8210\\AppData\\Local\\Temp\\ipykernel_3908\\2727622581.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace('NM', np.nan)\n"
     ]
    }
   ],
   "source": [
    "df_new = pd.DataFrame()\n",
    "df_new_company = pd.DataFrame()\n",
    "\n",
    "for year in [2021,2022]:\n",
    "    # Concatenate the DataFrames\n",
    "    df_year, df_company = data_processing(year)\n",
    "    df_new = pd.concat([df_new, df_year], ignore_index=True)\n",
    "    df_new_company = pd.concat([df_new_company, df_company], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old.to_excel(\"df_old.xlsx\")\n",
    "df_new.to_excel(\"df_new.xlsx\")\n",
    "df_old_company.to_excel(\"df_old_company.xlsx\")\n",
    "df_new_company.to_excel(\"df_new_company.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([80, 791])\n",
      "y_train shape: (80,)\n",
      "x_test shape: torch.Size([20, 791])\n",
      "y_test shape: (20,)\n",
      "x_train shape: torch.Size([80, 791])\n",
      "y_train shape: (80,)\n",
      "x_test shape: torch.Size([20, 791])\n",
      "y_test shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "for data in [\"old\", \"new\"]:\n",
    "\n",
    "    if data == \"old\":\n",
    "        df = df_old\n",
    "    else:\n",
    "        df = df_new\n",
    "        \n",
    "    # Sample 500 rows with a fixed random state\n",
    "    df = df.sample(n=100, random_state=42)\n",
    "    df.to_excel(f\"df_{data}_sample.xlsx\")\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # 分成文本和數值型\n",
    "    df_numeric = df.drop(\"text\", axis=1)\n",
    "    df_text = df[[\"text\",\"rate\"]]\n",
    "\n",
    "    ## 純數值\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "\n",
    "    X = df_numeric.drop('rate', axis=1)\n",
    "    y = le.fit_transform(df_numeric['rate'])\n",
    "\n",
    "    # 對 X 資料標準化\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    indices = range(100)\n",
    "    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, indices, test_size=0.2, random_state=42)\n",
    "    \n",
    "    ## 純文本\n",
    "    \n",
    "    # Split the DataFrame into training and test sets\n",
    "    df_train, df_test = train_test_split(df_text, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load the DistilBERT tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "    # Tokenize the text data for the training and validation sets\n",
    "    tokenized_train = tokenizer(df_train[\"text\"].values.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_test = tokenizer(df_test[\"text\"].values.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Pass the tokenized text through the DistilBERT model to get the hidden states\n",
    "    with torch.no_grad():\n",
    "        hidden_train = model(**tokenized_train)\n",
    "        hidden_test = model(**tokenized_test)\n",
    "\n",
    "    # Get only the [CLS] token hidden states\n",
    "    cls_train = hidden_train.last_hidden_state[:,0,:]\n",
    "    cls_test = hidden_test.last_hidden_state[:,0,:]\n",
    "\n",
    "    ## 數值 + 文本\n",
    "\n",
    "    # Concatenate the [CLS] token hidden states and the general features for the training set\n",
    "    x_train = torch.cat((cls_train, torch.from_numpy(X_train)), 1)\n",
    "    x_test = torch.cat((cls_test, torch.from_numpy(X_test)), 1)\n",
    "\n",
    "    # Print the shapes of the input and target tensors\n",
    "    print(f\"x_train shape: {x_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"x_test shape: {x_test.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "    # data & index storing\n",
    "    with open(f'x_train_{data}.pickle', 'wb') as handle:\n",
    "        pickle.dump(x_train, handle)\n",
    "    with open(f'y_train_{data}.pickle', 'wb') as handle:\n",
    "        pickle.dump(y_train, handle)\n",
    "    with open(f'x_test_{data}.pickle', 'wb') as handle:\n",
    "        pickle.dump(x_test, handle)\n",
    "    with open(f'y_test_{data}.pickle', 'wb') as handle:\n",
    "        pickle.dump(y_test, handle)     \n",
    "    with open(f\"indices_train_{data}\", \"wb\") as handle:\n",
    "        pickle.dump(indices_train, handle)        \n",
    "    with open(f\"indices_test_{data}\", \"wb\") as handle:\n",
    "        pickle.dump(indices_test, handle)          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
